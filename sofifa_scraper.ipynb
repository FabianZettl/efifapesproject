{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FabianZettl/efifapesproject/blob/main/sofifa_scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Cz6NhHaCnoj-",
        "outputId": "d0fece4e-ccbd-486b-e7c0-e2625fa6331c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Homepage Scrape\n",
            "2. Team Scrape\n",
            "3. Player Scrape\n",
            "4. \"Updated\" tab Scrape\n",
            "5. Scrape Filtered URLs\n",
            "6. Exit Code\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import requests\n",
        "import time\n",
        "import datetime\n",
        "from time import sleep\n",
        "import json\n",
        "import os \n",
        "import re\n",
        "headers = {\n",
        "    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'\n",
        "}\n",
        "\n",
        "\n",
        "def player_scraper(url):\n",
        "    list1 = []\n",
        "    r  = requests.get(url,headers = headers)\n",
        "    soup = bs(r.content,'html.parser')\n",
        "    short_name = soup.find('h1').text.strip()\n",
        "    try:\n",
        "        player_id = url.split('player/')[1].split('/')[0]\n",
        "    except:\n",
        "        player_id = url.split('player/')[1]\n",
        "    try:\n",
        "        ply_name = soup.find('div',attrs = {'class':'bp3-card player'}).find('h1').text\n",
        "    except:\n",
        "        ply_name = ''\n",
        "    try:\n",
        "        nationality = soup.find('div',attrs = {'class':'meta ellipsis'}).find('a')['title']\n",
        "    except:\n",
        "        nationality = ''\n",
        "    try:\n",
        "        h_w_text = soup.find('div',attrs = {'class':'meta ellipsis'}).text\n",
        "        height = h_w_text.split(' ')[-2]\n",
        "        if 'cm' not in height:\n",
        "            height = ''\n",
        "        weight = h_w_text.split(' ')[-1]\n",
        "        if 'kg' not in weight:\n",
        "            weight = ''\n",
        "        try:\n",
        "            birth_date = h_w_text.split('(')[1].split(')')[0]\n",
        "        except:\n",
        "            birth_date = ''\n",
        "        try:\n",
        "            pref_position = ''\n",
        "            for h_w in h_w_text:\n",
        "                if h_w.isnumeric():\n",
        "                    break\n",
        "                else:\n",
        "                    pref_position = pref_position + h_w\n",
        "        except:\n",
        "            pref_position = '' \n",
        "        try:\n",
        "            age = h_w_text.split('(')[0].split(' ')[-2]\n",
        "        except:\n",
        "            age = ''\n",
        "    except:\n",
        "        height = ''\n",
        "        weight = ''\n",
        "        \n",
        "    try:\n",
        "        cards = soup.find('section',attrs = {'class':'card spacing'}).findAll('div',recursive = False)\n",
        "        overall= cards[0].text.replace('Overall Rating','')\n",
        "        potential = cards[1].text.replace('Potential','')\n",
        "        value = cards[2].text.replace('Value','')\n",
        "        wage = cards[3].text.replace('Wage','')\n",
        "    except:\n",
        "        overall= ''\n",
        "        potential = ''\n",
        "        value = ''\n",
        "        wage = ''\n",
        "    try:\n",
        "        blocks = soup.find('div',attrs= {'class':'col col-12'}).findAll('div',recursive=False)\n",
        "        profile_block = blocks[1].find('div').find('ul').findAll('li')\n",
        "        pref_foot = profile_block[0].text.replace('Preferred Foot','').strip()\n",
        "        week_foot = profile_block[1].text.replace('Weak Foot','').strip()\n",
        "        skill_moves = profile_block[2].text.replace('Skill Moves','').strip()\n",
        "        work_rate = profile_block[4].text.replace('Work Rate','').strip()\n",
        "    except:\n",
        "        pref_foot = ''\n",
        "        week_foot = ''\n",
        "        skill_moves = ''\n",
        "        work_rate = ''\n",
        "        \n",
        "    try:\n",
        "        blocks = soup.find('div',attrs= {'class':'col col-12'}).findAll('div',recursive=False)\n",
        "        if len(blocks) ==6:\n",
        "            country_block = blocks[-2]\n",
        "            nation = country_block.find('h5').text\n",
        "            nation_position = country_block.find('ul').findAll('li')[1].text.replace('Position','')\n",
        "            nation_kit_num = country_block.find('ul').findAll('li')[2].text.replace('Kit Number','')\n",
        "            \n",
        "            \n",
        "            club_block = blocks[-3]\n",
        "            club = club_block.find('h5').text\n",
        "            club_lis = club_block.find('ul').findAll('li')\n",
        "            club_position = club_lis[1].text.replace('Position','')\n",
        "            club_kit = club_lis[2].text.replace('Kit Number','')\n",
        "            club_joined = club_lis[3].text.replace('Joined','')\n",
        "            if 'loan' in club_joined:\n",
        "                club_joined = club_joined.replace('Loaned From','') + '(Loaned)'\n",
        "            club_valid_until = club_lis[4].text.replace('Contract Valid Until','')\n",
        "        else:\n",
        "            nation = ''\n",
        "            nation_position = ''\n",
        "            nation_kit_num = ''\n",
        "            \n",
        "            \n",
        "            club_block = blocks[-2]\n",
        "            club_lis = club_block.find('ul').findAll('li')\n",
        "            \n",
        "            club = club_block.find('h5').text\n",
        "            club_position = club_lis[1].text.replace('Position','')\n",
        "            club_kit = club_lis[2].text.replace('Kit Number','')\n",
        "            club_joined = club_lis[3].text.replace('Joined','')\n",
        "            # if 'Loan' in club_joined:\n",
        "            #     club_joined = club_joined.replace('Loaned From') + '(Loaned)'\n",
        "            club_valid_until = club_lis[4].text.replace('Contract Valid Until','')\n",
        "    \n",
        "    except:\n",
        "        nation = ''\n",
        "        nation_position = ''\n",
        "        nation_kit_num = ''\n",
        "        \n",
        "        club = ''\n",
        "        club_position = ''\n",
        "        club_kit = ''\n",
        "        club_joined = ''\n",
        "        club_valid_until = ''\n",
        "    try:\n",
        "        blocks = soup.findAll('div',attrs= {'class':'col col-12'})[1].findAll('div',recursive = False)\n",
        "        attack_block = blocks[2].find('ul').findAll('li')\n",
        "        \n",
        "        crossing = attack_block[0].find('span').text\n",
        "        finishing = attack_block[1].find('span').text\n",
        "        heading_acc = attack_block[2].find('span').text\n",
        "        short_pass = attack_block[3].find('span').text\n",
        "        volleys = attack_block[4].find('span').text\n",
        "        #-----------------------------------------------\n",
        "        skill_block = blocks[3].find('ul').findAll('li')\n",
        "        \n",
        "        dribbling = skill_block[0].find('span').text\n",
        "        curve = skill_block[1].find('span').text\n",
        "        fk_accuracy = skill_block[2].find('span').text\n",
        "        long_pass = skill_block[3].find('span').text\n",
        "        ball_control = skill_block[4].find('span').text\n",
        "        #-----------------------------------------------\n",
        "        movement_block = blocks[4].find('ul').findAll('li')\n",
        "        \n",
        "        acceleration = movement_block[0].find('span').text\n",
        "        sprint_speed = movement_block[1].find('span').text\n",
        "        agility = movement_block[2].find('span').text\n",
        "        reactions = movement_block[3].find('span').text\n",
        "        balance = movement_block[4].find('span').text\n",
        "        #-----------------------------------------------\n",
        "        power_block = blocks[5].find('ul').findAll('li')\n",
        "\n",
        "        shot_power = power_block[0].find('span').text\n",
        "        jumping = power_block[1].find('span').text\n",
        "        stamina = power_block[2].find('span').text\n",
        "        strength = power_block[3].find('span').text\n",
        "        long_shot = power_block[4].find('span').text\n",
        "        #-----------------------------------------------\n",
        "        mentality_block = blocks[6].find('ul').findAll('li')\n",
        "        \n",
        "        aggresions = mentality_block[0].find('span').text\n",
        "        interceptions = mentality_block[1].find('span').text\n",
        "        positioning = mentality_block[2].find('span').text\n",
        "        vision = mentality_block[3].find('span').text\n",
        "        penalties = mentality_block[4].find('span').text\n",
        "        composure = mentality_block[5].find('span').text\n",
        "        #-----------------------------------------------\n",
        "        defending_block = blocks[7].find('ul').findAll('li')\n",
        "        \n",
        "        def_awareness = defending_block[0].find('span').text\n",
        "        standing_tackle = defending_block[1].find('span').text\n",
        "        sliding_tackle = defending_block[2].find('span').text\n",
        "  \n",
        "        #-----------------------------------------------\n",
        "        goal_block = blocks[8].find('ul').findAll('li')\n",
        "        \n",
        "        gk_diving = goal_block[0].find('span').text\n",
        "        gk_handling = goal_block[1].find('span').text\n",
        "        gk_kicking = goal_block[2].find('span').text\n",
        "        gk_position = goal_block[3].find('span').text\n",
        "        gk_reflex = goal_block[4].find('span').text\n",
        "        #--------traits---------------------------------------\n",
        "        try:\n",
        "            trait_block = blocks[9].find('ul').findAll('li')\n",
        "            \n",
        "            traits = ''\n",
        "            for tra in trait_block:\n",
        "                traits = traits +','+ tra.text\n",
        "            if traits[-1] == ',':\n",
        "                traits = traits[:-1]\n",
        "            if traits[0] == ',':\n",
        "                traits = traits[1:]\n",
        "        except:\n",
        "            traits = ''\n",
        "    except:\n",
        "        crossing = ''\n",
        "        finishing = ''\n",
        "        heading_acc = ''\n",
        "        short_pass = ''\n",
        "        volleys = ''\n",
        "        dribbling = ''\n",
        "        curve = ''\n",
        "        fk_accuracy = ''\n",
        "        long_pass = ''\n",
        "        ball_control = ''\n",
        "        acceleration = ''\n",
        "        sprint_speed = ''\n",
        "        agility = ''\n",
        "        reactions = ''\n",
        "        balance = ''\n",
        "        aggresions = ''\n",
        "        interceptions = ''\n",
        "        positioning = ''\n",
        "        vision = ''\n",
        "        penalties = ''\n",
        "        composure = ''\n",
        "        def_awareness = ''\n",
        "        standing_tackle = ''\n",
        "        sliding_tackle = ''\n",
        "        gk_diving = ''\n",
        "        gk_handling = ''\n",
        "        gk_kicking = ''\n",
        "        gk_position = ''\n",
        "        gk_reflex = ''\n",
        "        traits = ''\n",
        "        shot_power = ''\n",
        "        jumping = ''\n",
        "        stamina = ''\n",
        "        strength = ''\n",
        "        long_shot = ''\n",
        "    datan = {\n",
        "        'Profile URL': url,\n",
        "        'Player ID': player_id,\n",
        "        'Player Name': ply_name,\n",
        "        'Player Short Name': short_name,\n",
        "        'Nationality':nationality,\n",
        "        'Overall': overall,\n",
        "        'Potential': potential,\n",
        "        'Height': height,\n",
        "        'Weight': weight,\n",
        "        'PreferredFoot': pref_foot,\n",
        "        'BirthDate': birth_date,\n",
        "        'Age': age,\n",
        "        'PreferredPositions': pref_position,\n",
        "        'PlayerWorkRate': work_rate,\n",
        "        'WeekFoot': week_foot,\n",
        "        'SkillMoves': skill_moves,\n",
        "        'Value': value,\n",
        "        'Wage': wage,\n",
        "        'Nation': nation,\n",
        "        'Nation_Position': nation_position,\n",
        "        'NationKitNumber': nation_kit_num,\n",
        "        'Club': club,\n",
        "        'Club_Position': club_position,\n",
        "        'Club_KitNumber': club_kit,\n",
        "        'Club_Joined': club_joined,\n",
        "        'Club_ContractLength': club_valid_until,\n",
        "        'Crossing': crossing,\n",
        "        'Finishing': finishing,\n",
        "        'Heading Accuracy': heading_acc,\n",
        "        'Short Passing': short_pass,\n",
        "        'Volleys': volleys,\n",
        "        'Dribbling': dribbling,\n",
        "        'Curve': curve,\n",
        "        'FK Accuracy': fk_accuracy,\n",
        "        'Long Passing': long_pass,\n",
        "        'Ball Control': ball_control,\n",
        "        'Acceleration': acceleration,\n",
        "        'Sprint Speed': sprint_speed,\n",
        "        'Agility': agility,\n",
        "        'Reactions': reactions,\n",
        "        'Balance': balance,\n",
        "        'Shot Power': shot_power,\n",
        "        'Jumping': jumping,\n",
        "        'Stamina': stamina,\n",
        "        'Strength':strength,\n",
        "        'Long Shot': long_shot,\n",
        "        'Aggression': aggresions,\n",
        "        'Interceptions': interceptions,\n",
        "        'Positioning': positioning,\n",
        "        'Vision': vision,\n",
        "        'Penalties': penalties,\n",
        "        'Composure': composure,\n",
        "        'Defensive Awareness': def_awareness,\n",
        "        'Standing Tackle': standing_tackle,\n",
        "        'Sliding Tackle': sliding_tackle,\n",
        "        'GK Diving': gk_diving,\n",
        "        'GK Handling': gk_handling,\n",
        "        'GK Kicking': gk_kicking,\n",
        "        'GK Positioning': gk_position,\n",
        "        'GK Reflexes': gk_reflex,\n",
        "        'Traits' :traits\n",
        "    }\n",
        "    list1.append(datan)\n",
        "    \n",
        "    return list1\n",
        "    \n",
        "# https://sofifa.com/players?type=updated\n",
        "\n",
        "def main():\n",
        "    print('1. Homepage Scrape')\n",
        "    print('2. Team Scrape')\n",
        "    print('3. Player Scrape')\n",
        "    print('4. \"Updated\" tab Scrape')\n",
        "    print('5. Scrape Filtered URLs')\n",
        "    print('6. Exit Code')\n",
        "    choice = int(input('Enter Choice(1-6): '))\n",
        "    if choice==3:\n",
        "        url = input('Enter Player Url: ')\n",
        "        listan = player_scraper(url)\n",
        "\n",
        "        try:\n",
        "            player_id = url.split('player/')[1].split('/')[0]\n",
        "        except:\n",
        "            player_id = url.split('player/')[1]\n",
        "        df = pd.DataFrame(listan).to_excel(f'player_id_{player_id}.xlsx',encoding = 'utf-8-sig',index = False)\n",
        "        from google.colab import files\n",
        "        files.download(f'player_id_{player_id}.xlsx')\n",
        "    elif choice == 1:\n",
        "        player_urls = []\n",
        "        listan = []\n",
        "        count = 60\n",
        "        while True:\n",
        "            if count == 20060:\n",
        "                break\n",
        "            prev_len = len(player_urls)\n",
        "            try:\n",
        "                url = f'https://sofifa.com/?offset={count}'\n",
        "                print(f'Scraping Players URL from Homepage: {url}')\n",
        "                r  = requests.get(url,headers = headers)\n",
        "                soup = bs(r.content,'html.parser')\n",
        "\n",
        "                pr = soup.find('table',attrs = {'class':'table table-hover persist-area'}).findAll('tr')[1:]\n",
        "                for p in pr:\n",
        "                    try:\n",
        "                        player_url = 'https://sofifa.com' +  str(p.findAll('td')[1].find('a')['href'])\n",
        "                        if player_url in player_urls:\n",
        "                            pass\n",
        "                        else:\n",
        "                            player_urls.append(player_url)\n",
        "                    except:\n",
        "                        pass\n",
        "            \n",
        "            except:\n",
        "                pass\n",
        "            if len(player_urls) == prev_len:\n",
        "                break\n",
        "            count = count + 60\n",
        "        sleep(2)\n",
        "        print(f'\\nTotal Players Profile found from homepage: {len(player_urls)}\\n')\n",
        "        sleep(3)\n",
        "        print('Scraping Starts:')\n",
        "\n",
        "        progress = 1\n",
        "        totals = len(player_urls)\n",
        "        for url in player_urls:\n",
        "            print(f'[{progress}/{totals}] Scraping Player URL: {url}')\n",
        "            list2 = player_scraper(url)\n",
        "            listan = listan + list2\n",
        "            progress = progress + 1\n",
        "        \n",
        "        df = pd.DataFrame(listan).to_excel(f'players_from_homepage.xlsx',encoding = 'utf-8-sig',index = False)\n",
        "        from google.colab import files\n",
        "        files.download(f'players_from_homepage.xlsx')\n",
        "    #---adding----------\n",
        "    elif choice == 4:\n",
        "        player_urls = []\n",
        "        listan = []\n",
        "        count = 60\n",
        "        while True:\n",
        "            if count == 20060:\n",
        "                break\n",
        "            prev_len = len(player_urls)\n",
        "            try:\n",
        "                url = f'https://sofifa.com/players?type=updated&offset={count}'\n",
        "                print(f'Scraping Players URL from Update Tab: {url}')\n",
        "                r  = requests.get(url,headers = headers)\n",
        "                soup = bs(r.content,'html.parser')\n",
        "\n",
        "                pr = soup.find('table',attrs = {'class':'table table-hover persist-area'}).findAll('tr')[1:]\n",
        "                for p in pr:\n",
        "                    try:\n",
        "                        player_url = 'https://sofifa.com' +  str(p.findAll('td')[1].find('a')['href'])\n",
        "                        if player_url in player_urls:\n",
        "                            pass\n",
        "                        else:\n",
        "                            player_urls.append(player_url)\n",
        "                    except:\n",
        "                        pass\n",
        "            except:\n",
        "                pass\n",
        "            if len(player_urls) == prev_len:\n",
        "                break\n",
        "            count = count + 60\n",
        "        sleep(2)\n",
        "        print(f'\\nTotal Players Profile found from Update Tab: {len(player_urls)}\\n')\n",
        "        sleep(3)\n",
        "        print('Scraping Starts:')\n",
        "\n",
        "        progress = 1\n",
        "        totals = len(player_urls)\n",
        "        for url in player_urls:\n",
        "            print(f'[{progress}/{totals}] Scraping Player URL: {url}')\n",
        "            list2 = player_scraper(url)\n",
        "            listan = listan + list2\n",
        "            progress = progress + 1\n",
        "        \n",
        "        df = pd.DataFrame(listan).to_excel(f'players_from_update_tab.xlsx',encoding = 'utf-8-sig',index = False)\n",
        "        from google.colab import files\n",
        "        files.download(f'players_from_update_tab.xlsx')\n",
        "    elif choice == 5:\n",
        "        filter_url = input('Enter Filtered URL: ')\n",
        "        player_urls = []\n",
        "        listan = []\n",
        "        count = 60\n",
        "        while True:\n",
        "            if count == 20060:\n",
        "                break\n",
        "            prev_len = len(player_urls)\n",
        "            try:\n",
        "                url = f'{filter_url}&offset={count}'\n",
        "                print(f'Scraping Filtered Players URL: {url}')\n",
        "                r  = requests.get(url,headers = headers)\n",
        "                soup = bs(r.content,'html.parser')\n",
        "\n",
        "                pr = soup.find('table',attrs = {'class':'table table-hover persist-area'}).findAll('tr')[1:]\n",
        "                for p in pr:\n",
        "                    try:\n",
        "                        player_url = 'https://sofifa.com' +  str(p.findAll('td')[1].find('a')['href'])\n",
        "                        if player_url in player_urls:\n",
        "                            pass\n",
        "                        else:\n",
        "                            player_urls.append(player_url)\n",
        "                    except:\n",
        "                        pass\n",
        "            except:\n",
        "                pass\n",
        "            if len(player_urls) == prev_len:\n",
        "                break\n",
        "            count = count + 60\n",
        "        sleep(2)\n",
        "        print(f'\\nTotal Filtered Players Profile found: {len(player_urls)}\\n')\n",
        "        sleep(3)\n",
        "        print('Scraping Starts:')\n",
        "\n",
        "        progress = 1\n",
        "        totals = len(player_urls)\n",
        "        for url in player_urls:\n",
        "            print(f'[{progress}/{totals}] Scraping Player URL: {url}')\n",
        "            list2 = player_scraper(url)\n",
        "            listan = listan + list2\n",
        "            progress = progress + 1\n",
        "        \n",
        "        df = pd.DataFrame(listan).to_excel(f'players_from_filtered_url.xlsx',encoding = 'utf-8-sig',index = False)\n",
        "        from google.colab import files\n",
        "        files.download(f'players_from_filtered_url.xlsx')\n",
        "    #----adding-done---------------------------------\n",
        "\n",
        "    elif choice == 2:\n",
        "        team_url = input('Enter Team Url(ex. https://sofifa.com/team/1/arsenal):  ')\n",
        "        listan = []\n",
        "        r  = requests.get(team_url,headers = headers)\n",
        "        soup = bs(r.content,'html.parser')\n",
        "        player_block = soup.findAll('table',attrs = {'class':'table table-hover persist-area'})\n",
        "        player_urls = []\n",
        "        squads_trs = player_block[0].findAll('tr')\n",
        "        for tr in squads_trs:\n",
        "            try:\n",
        "                player_url = 'https://sofifa.com' + str(tr.findAll('td')[1].find('a')['href'])\n",
        "                player_urls.append(player_url)\n",
        "            except:\n",
        "                pass\n",
        "        loan_trs = player_block[1].findAll('tr')\n",
        "        for tr in loan_trs:\n",
        "            try:\n",
        "                player_url = 'https://sofifa.com' + str(tr.findAll('td')[1].find('a')['href'])\n",
        "                player_urls.append(player_url)\n",
        "            except:\n",
        "                pass\n",
        "        progress = 1\n",
        "        totals = len(player_urls)\n",
        "        for url in player_urls:\n",
        "            print(f'[{progress}/{totals}] Scraping Player URL: {url}')\n",
        "            list2 = player_scraper(url)\n",
        "            listan = listan + list2\n",
        "            progress = progress + 1\n",
        "        \n",
        "        team_name = team_url.split('/')[-1]\n",
        "        df = pd.DataFrame(listan).to_excel(f'team_name_{team_name}.xlsx',encoding = 'utf-8-sig',index = False)\n",
        "        from google.colab import files\n",
        "        files.download(f'team_name_{team_name}.xlsx')\n",
        "    else:\n",
        "        exit()\n",
        "main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}